{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55e07ac",
   "metadata": {},
   "source": [
    "# Pipeline For CoT Validation\n",
    "\n",
    "This pipeline will hold all :) - unfinished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8840521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from src.reasoning.formatting import apply_instruct_format\n",
    "from src.reasoning.inference import run_inference\n",
    "from src.reasoning.certainty_calc import capture_confidence_tokens\n",
    "from src.utils.memory_management import check_memory\n",
    "from src.utils.data_handler import batch_generate, tokens_generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd62b3",
   "metadata": {},
   "source": [
    "## Load Config, Dataset & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ecd13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "model_str = 'deepseek_r1_qwendistill_14'\n",
    "ds_str = 'big_bench'\n",
    "ds_config = config['datasets'][ds_str]\n",
    "model_path = config['models'][model_str]['path']\n",
    "\n",
    "# Decide on device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "check_memory(device=device)\n",
    "\n",
    "# Load dataset and model\n",
    "dataset = load_dataset(ds_config['source'], ds_config['subset']).shuffle(config['seed'])\n",
    "df = pd.DataFrame(dataset['train'][:5])  # small slice for now ! CHANGE\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cde9eb",
   "metadata": {},
   "source": [
    "## Set Up Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841da857",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompts = apply_instruct_format(\n",
    "    inputs=df[ds_config['input_column']].tolist(),\n",
    "    config=config,\n",
    "    model=model_str,\n",
    "    is_thinking=True,\n",
    "    is_answer_format=True\n",
    ")\n",
    "df['formatted_prompt'] = formatted_prompts\n",
    "\n",
    "# Batch + tokenize\n",
    "batches = batch_generate(df, ds_config['input_column'], ds_config['target_column'], batch_size=2)\n",
    "tokenized_data = tokens_generate(batches, tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e88c00b",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_inference(model, tokenized_data, tokenizer, time_tracking=True, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f6695",
   "metadata": {},
   "source": [
    "For each generated output, do chain-of-thought probability capture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d478a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_analysis = []\n",
    "for output_dict in results:\n",
    "    # For now: single example at a time! -will iterate inside them as well\n",
    "    gen_tokens = output_dict['response']  # This is the decoded string. \n",
    "    # eed the token IDs - re-encoding them:\n",
    "    gen_token_ids = tokenizer.encode(gen_tokens, return_tensors='pt').to(device)\n",
    "\n",
    "    # Build a single input_ids tensor from the original\n",
    "    # 'tokenized_prompts' is already on device. It's a dict with input_ids, attention_mask, etc.\n",
    "    input_ids = output_dict[\"tokenized_prompts\"][\"input_ids\"]  # shape [batch_size, seq_len]\n",
    "    \n",
    "    # For simplicity, pass them as shape [1, seq_len] each (will have multiple samples in batch -> loop them)\n",
    "    single_input_ids = input_ids[0].unsqueeze(0)\n",
    "    single_gen_tokens = gen_token_ids[0].unsqueeze(0)\n",
    "\n",
    "    # knowing correct answer is \"Sam\" / can read it from output_dict[\"target\"] if it's a single token label\n",
    "    correct_answer_str = \"Sam\"\n",
    "\n",
    "    chain_results = capture_confidence_tokens(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        input_ids=single_input_ids,\n",
    "        generated_tokens=single_gen_tokens,\n",
    "        correct_answer_str=correct_answer_str,\n",
    "        threshold=0.90,\n",
    "        topk=1000,\n",
    "        device=device\n",
    "    )\n",
    "    cot_analysis.append(chain_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b24aed",
   "metadata": {},
   "source": [
    "## Examine chain_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e611be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, analysis in enumerate(cot_analysis):\n",
    "    print(f\"\\nExample {i} final answer: {analysis['final_answer']}\")\n",
    "    print(f\"Step at which prob >= 90%: {analysis['step_90_confidence']}\")\n",
    "    # analysis['step_details'] is a list with token-by-token info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envthr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
